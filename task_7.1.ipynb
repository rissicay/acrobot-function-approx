{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8646db",
   "metadata": {},
   "source": [
    "# Task 7.1D Function Approximation Implementation\n",
    "Christopher Abbey\n",
    "\n",
    "This report will provide implementation of the the following methods:\n",
    "- Semi-Gradient Sarsa(0)\n",
    "- Semi-Gradient TD(Lambda)\n",
    "\n",
    "We will implement both algorithms and then compare the similarity and differences of these algorithm. Semi-Gradient Sarsa will then be used to solve Acrobot-v1. Finally, compare these two algorithms, this includes there prupose, there similarities and there differences.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "We are reusing the environment created in Task 3.1D. This environment we have exposed the raw state variables, instead of having the state variables being past into trig functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfe3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c15ab",
   "metadata": {},
   "source": [
    "## State Aggregation\n",
    "\n",
    "In Task 3.1D I had used tile coding, without knowing what tile coding was. This code is being reused, while the naming is different than what is typically used in tile coding, the principals are the same and I am very similar with the code. I have increased the state count slightly higher than Task 3.1, mainly because I could, the implementation of this task is more memory efficient than Task 3.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a14281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_point(value, increment):\n",
    "    return round(round(value / increment) * increment, 2)\n",
    "\n",
    "def approximate(raw_state):\n",
    "    array = list(raw_state)\n",
    "    return tuple([\n",
    "        find_point(array[0], 0.2) + 0, \n",
    "        find_point(array[1], 0.2) + 0, \n",
    "        find_point(array[2], 0.2) + 0,\n",
    "        find_point(array[3], 0.5) + 0\n",
    "    ])\n",
    "\n",
    "def generate(lower_bound, upper_bound, interval):\n",
    "    output = [lower_bound]\n",
    "    while lower_bound <= upper_bound:\n",
    "        lower_bound += interval\n",
    "        output.append( float( round(lower_bound,1) + 0 ) )\n",
    "    return output\n",
    "\n",
    "def generate_states(theta1, theta2, ang_vel_theta1, ang_vel_theta2):\n",
    "    states = []\n",
    "    has_zero = False\n",
    "    for a in theta1:\n",
    "        for b in theta2:\n",
    "            for c in ang_vel_theta1:\n",
    "                for d in ang_vel_theta2:\n",
    "                    if a == 0.0 and b == 0.0 and c == 0.0 and d == 0.0:\n",
    "                        has_zero = True\n",
    "                    states.append((a, b, c, d))\n",
    "                    \n",
    "    if not has_zero:\n",
    "        states.append((0.0, 0.0, 0.0, 0.0))\n",
    "                    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d650339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acrobot_env import *\n",
    "\n",
    "env = AcrobotEnv()\n",
    "\n",
    "theta1 = generate(-3.2, 3.2, 0.2)\n",
    "theta2 = generate(-3.2, 3.2, 0.2)\n",
    "ang_vel_theta1 = generate(-13, 13, 0.2)\n",
    "ang_vel_theta2 = generate(-28.5, 28.5, 0.5)\n",
    "\n",
    "states = generate_states(theta1, theta2, ang_vel_theta1, ang_vel_theta2)\n",
    "\n",
    "mapping = {}\n",
    "idx = 0\n",
    "for state in states:\n",
    "    mapping[state] = idx\n",
    "    idx += 1\n",
    "    \n",
    "mapping_state_action = {}\n",
    "idx = 0\n",
    "\n",
    "actions = [0, 1, 2]\n",
    "\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        mapping_state_action[(state, action)] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40e739",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD(Lambda)\n",
    "\n",
    "![Pseudocode for Semi-Gradient TD(Lambda)](img/td.png)\n",
    "\n",
    "Semi-Gradient Td(Lambda) is a prediction algorithm, not a control algorithm. A prediction algorithm takes a policy and returns a value function. The policy we have selected to evaluation is a random policy. Since a value function is returned from this algorithm we can not use it to solve acrobot. It is possible to edit this algorithm to return a q function instead of a value function, this is a different algorithm though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2955b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 10**-8\n",
    "\n",
    "class TDEstimator():\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.9, lamb=0.8):\n",
    "        self.weights = np.zeros(len(mapping))\n",
    "        self.z = np.zeros(len(mapping))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def featurize_state(self, state):\n",
    "        point = approximate(state)\n",
    "        idx = mapping[point]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \n",
    "        idx = self.featurize_state(state)\n",
    "        return self.weights[idx]\n",
    "    \n",
    "    def update_trace(self, state):\n",
    "        dev = 1\n",
    "        self.z *= self.gamma * self.lamb\n",
    "        \n",
    "        idx = self.featurize_state(state)\n",
    "        self.z[idx] += dev\n",
    "    \n",
    "    def update(self, next_state, state, reward):\n",
    "        delta = reward + self.gamma * self.predict(next_state) - self.predict(state)\n",
    "        delta *= self.alpha\n",
    "        self.weights += delta * self.z\n",
    "        self.weights[np.isclose(self.weights, 0, atol=tolerance)] = 0 # to prevent stuff from overflowing\n",
    "        \n",
    "\n",
    "def get_random_action():\n",
    "    return np.random.choice([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563d1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_semi_grad_td = []\n",
    "\n",
    "def semi_grad_td_policy_eval(env, episodes=100, alpha=0.1, gamma=0.9, lamb=0.8):\n",
    "    print(\"start\")\n",
    "    estimator = TDEstimator(alpha=alpha, gamma=gamma, lamb=lamb)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        reward_acc = 0\n",
    "        state = env.reset()\n",
    "        action = get_random_action()\n",
    "        \n",
    "        for t_ in range(10**100):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            reward_acc += reward\n",
    "            \n",
    "            \n",
    "            estimator.update_trace(state)\n",
    "            estimator.update(state, next_state, reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                rewards_semi_grad_td.append(reward_acc)\n",
    "                print(\"completed an episode\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"step {t_}\", end='\\r')\n",
    "                \n",
    "            action = get_random_action()\n",
    "                \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa082e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "completed an episode\n",
      "step 678\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43msemi_grad_td_policy_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36msemi_grad_td_policy_eval\u001b[0;34m(env, episodes, alpha, gamma, lamb)\u001b[0m\n\u001b[1;32m     13\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m reward_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 17\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m estimator\u001b[38;5;241m.\u001b[39mupdate(state, next_state, reward)\n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTDEstimator.update_trace\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     24\u001b[0m     dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamb\n\u001b[1;32m     27\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturize_state(state)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dev\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator = semi_grad_td_policy_eval(env, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374c98a",
   "metadata": {},
   "source": [
    "## Semi-Gradient Sarsa(0)\n",
    "\n",
    "![Pseudocode for Semi-Gradient Sarsa(0)](img/sarsa.png)\n",
    "\n",
    "Semi-Gradient Sarsa is a control algorithm. This algorithm will generate an optimal policy that can be used to solve acrobot. The implement below is standard implementation. Since tile coding is used, the predict function for the estimator simplifies to just the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da84e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(state, q_estimator, eps=0.01):\n",
    "    if np.random.rand() > eps:\n",
    "        q_values = [q_estimator.predict(state,a) for a in actions]\n",
    "        return argmax_rand(q_values)\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "\n",
    "def argmax_rand(arr):\n",
    "    # break ties randomly, np.argmax() always picks first max\n",
    "    return np.random.choice(np.flatnonzero(arr == np.max(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e48111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaEstimator():\n",
    "    \"\"\"\n",
    "    Linear action-value (q-value) function approximator for \n",
    "    semi-gradient methods with state-action featurization via tile coding. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.9):\n",
    "        \n",
    "        self.weights = np.zeros(len(mapping_state_action))\n",
    "        self.z = np.zeros(len(mapping_state_action))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        \n",
    "    def featurize_state_action(self, state, action):\n",
    "        point = approximate(state)\n",
    "        idx = mapping_state_action[(point, action)]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        return self.weights[self.featurize_state_action(s, a)]\n",
    "\n",
    "        \n",
    "            \n",
    "    def update(self, s, a, target):\n",
    "        features = self.featurize_state_action(s, a)\n",
    "        estimation = np.sum(self.weights[features])\n",
    "        delta = (target - estimation)\n",
    "        \n",
    "        self.weights += self.alpha * delta \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe811bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_semi_grad_sarsa = []\n",
    "\n",
    "def semi_grad_sarsa(env, episodes=100, alpha=0.1, gamma=0.9):\n",
    "\n",
    "    estimator = SarsaEstimator(alpha=alpha, gamma=gamma)\n",
    "    \n",
    "    \n",
    "    for e in range(episodes):\n",
    "        rewards_acc = 0\n",
    "        state = env.reset()\n",
    "        action = epsilon_greedy_action(state, estimator)\n",
    "        \n",
    "        for t_ in range(10**100):\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            rewards_acc += reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                estimator.update(state, action, reward)\n",
    "                rewards_semi_grad_sarsa.append(rewards_acc)\n",
    "                break\n",
    "\n",
    "                \n",
    "            action_next = epsilon_greedy_action(state, estimator)\n",
    "            target = reward + gamma * estimator.predict(state_next, action_next)\n",
    "            estimator.update(state, action, target)\n",
    "            state, action = state_next, action_next\n",
    "                \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = semi_grad_sarsa(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "\n",
    "for state in states:\n",
    "    policy[state] = argmax_rand([estimator.predict(state,a) for a in actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a895ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards = []\n",
    "for e in range(2):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(10**100):\n",
    "        point = approximate(state)\n",
    "        action = policy[point]\n",
    "        state, r, done, _ = env.step(action)  # Take step\n",
    "        env.render()  # Animate\n",
    "        reward += r\n",
    "        if done:\n",
    "            print('Solved in {} steps'.format(t))\n",
    "            rewards.append(reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaeba37",
   "metadata": {},
   "source": [
    "## Semi-Gradient Sarsa(0) Performance\n",
    "\n",
    "Like in Task 3.1, the policy produced is capable of solving acrobot. The average number of steps to complete acrobot is worst than the policy we produced in Task 3.1, even with more states. The big difference between Task 3.1 and Task 7.1 is the amount of training time required, Task 7.1 was significantly faster. To train the policy in Task 7.1 too multiple hours, on the other hand, Task 3.1 required me to run the algorithm overnight.\n",
    "\n",
    "It is of my opinion that we could match the performacne of Task 3.1 by increasing the episode we train for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = range(1, len(rewards_semi_grad_sarsa) + 1)\n",
    "\n",
    "plt.plot(episodes, rewards_semi_grad_td, 'bo', label='Semi-Gradient TD')\n",
    "plt.plot(episodes, rewards_semi_grad_sarsa, 'b', label='Semi-Gradient Sarsa')\n",
    "plt.title('Reward per episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de93a6f",
   "metadata": {},
   "source": [
    "## Comparison between Semi-Gradient TD(Lambda) and Semi-Gradient "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
